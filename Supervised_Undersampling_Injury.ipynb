{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Injury vs. Non-Injury Plays\n",
    "\n",
    "The issue we ran into with these data is that there are already 76 million rows in the tracking data, and merging additional columns is problematic in local analysis due to memory constraints. The plan for this analysis is to use undersampling from the outer merge of the Playlist-Injury Datasets, to randomly reduce the non-injury plays. It's important to perform this step at this time, so that we don't have to perform additional aggregation steps to the large table with 76 million rows. When we merge the Playlist-Injury dataset to the Tracking data, only the rows that match a PlayKey number will be merged, significantly cutting down the size of the dataframe by rows, as we increase the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NFL_Injury_Cleaning_Functions import *\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the datasets and Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = pd.read_csv(\"NFL_Turf/PlayList.csv\")\n",
    "injuries = pd.read_csv(\"NFL_Turf/InjuryRecord.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = ML_Data_Cleaner(playlist, injuries)\n",
    "ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml.set_index('PlayKey', inplace=True)\n",
    "ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adding one additional column, 'IsInjured', where it is 1 wherever the injury type is not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy where function reads as follows... set ml.IsInjured equal to 0 \n",
    "# where ml.InjuryType == 0, else set equal to 1. All injuryType 0 values are not injures,\n",
    "# everything else is an injury\n",
    " \n",
    "ml['IsInjured'] = np.where(ml['InjuryType'] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling\n",
    "\n",
    "We will undersample the data using the Cluster Centroids algorithm doing the following: \n",
    "\n",
    "1. View the count of the target class (injury types) using Counter from the collections library\n",
    "2. Use the resampled data to merge with the training data, only keeping the values that match the PlayKey from the sampled set\n",
    "3. Use the new dataset to perform machine learning analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ml.drop(columns=['InjuryType', 'InjuryDuration', 'SevereInjury'])\n",
    "y = ml.IsInjured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=seed)\n",
    "\n",
    "# Fit the resample\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Undersampled Data with the Tracking Data\n",
    "\n",
    "At this point, X_resampled and y_resampled have as many non-injury datapoints as it has injury datapoints, which will be expaneded once we add the tracking data. Note, these data have not been split using the train_test_split, as we still need to merge with the tracking data. The merge will be on the PlayKey, which will have to be an Inner merge, which will include all of the position data per play, but it will only contain the plays from the sampled data. \n",
    "\n",
    "Note: The X_resampled df still contains the IsInjured Column (y) - this is being maintained because after the merge with the tracking data, we will separate the y-values from the full table, to ensure that there wasn't some kind of indexing issue, so y_resampled is unnecessary moving forward.  \n",
    "\n",
    "First, load the tracking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking = pd.read_csv('NFL_Turf/PlayerTrackData.csv')\n",
    "tracking.drop(columns=['event', 'dis', 'time'], inplace=True)\n",
    "tracking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_merged = pd.merge(tracking, X_resampled, on='PlayKey', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_merged.PlayKey.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has reduced the number of rows from 76 million to 44 thousand, sampling from 153 different plays, 77 of which involve injuries. \n",
    "\n",
    "## Machine Learning Model\n",
    "\n",
    "The data will be split using train_test_split, and then similar to the previous models, a RandomForest classifier will be used for the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing \n",
    "\n",
    "X_merged = ml_merged.drop(columns=['PlayKey', 'IsInjured'])\n",
    "y_merged = ml_merged.IsInjured\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_merged, y_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Classifier\n",
    "barf = BalancedRandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "\n",
    "# Fit the model\n",
    "barf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate predicted accuracy score\n",
    "y_pred = barf.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_confusion_matrix(barf, X_test, y_test, display_labels=[\n",
    "                      \"Not Injured\", \"Injured\"], cmap='Blues', values_format='d', ax=ax)\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This peformed very will with the undersampling algorithm, but to better analyze overfitting issues, we also tested SMOTEENN, a combination of under and oversampling, to get a larger dataset to pull from.\n",
    "\n",
    "Note: Simple Logistic Regression was performed, which yielded only a balanced accuracy of 68%, just to verify that a more complex model should be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_smoteenn = ml.copy()\n",
    "\n",
    "ml_smoteenn.set_index('PlayKey', drop=True, inplace=True)\n",
    "ml_smoteenn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ml_smoteenn.drop(columns=['InjuryType', 'InjuryDuration', 'SevereInjury'])\n",
    "y = ml_smoteenn.IsInjured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoteenn = SMOTEENN(random_state=seed)\n",
    "X_resampled, y_resampled = smoteenn.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, this has exponentially more data than the Undersampling model, with 260,000 compared to 77 values per category\n",
    "\n",
    "## Merging the SMOTEENN resampled data with the Tracking Data\n",
    "\n",
    "Again, X_resampled and y_resampled have not been split into testing/training sets, as they will need to be merged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55a128abeef29120eb7bd22a6a2e883184ae9f9178564f0df9c7c7d3d676105f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

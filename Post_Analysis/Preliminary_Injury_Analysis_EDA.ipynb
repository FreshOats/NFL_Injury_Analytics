{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Clean and Analysis - Lower Body Injuries\n",
    "- **Preliminary Data Cleaning and Feature Analysis**\n",
    "- **Preliminary Machine Learning Models**\n",
    "\n",
    "This is a combined file containing the full preliminary analysis, which led to several changes implemented in the final data cleaning and machine learning models\n",
    "\n",
    "---\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from InjuryCleaningFunctions import column_capitalizer\n",
    "\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The processes used in the data cleaning only required Pandas and Numpy for the Python Processing. \n",
    "- The Scikit Learn library was used for the validation splits and encoding of dummy data\n",
    "- SQL Alchemy was used to connect to our database for both data retrieval and data exports \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis - Lower Body Injury Data\n",
    "### PlayList.csv\n",
    "\n",
    "The data provided from Kaggle in the .csv file titled \"PlayList.csv\" was imported into our PostgreSQL database named \"NFL_Turf\". The code below connects to the database and retreives the file and stores it as a dataframe titled \"playlist\"\n",
    "\n",
    "The first thing to note is that this list contains all of the plays, including the exact play that will match with the injury list, therefore anything that is on both with the exception of the PlayerKey should be maintained on THIS DF so that we don't lose data on the non-injury columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make connection to the database\n",
    "from config import db_password\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5433/NFL_Turf\"\n",
    "engine = db.create_engine(db_string)\n",
    "conn = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "\n",
    "del db_password\n",
    "\n",
    "# Read in the specific table - this can be done on the same connection:\n",
    "table = db.Table('playlist', metadata,\n",
    "                        autoload=True, autoload_with=engine)\n",
    "query = db.select(table)\n",
    "Results = conn.execute(query).fetchall()\n",
    "\n",
    "# Create the new dataframe and set the keys\n",
    "playlist = pd.DataFrame(Results)\n",
    "playlist.columns = Results[0].keys()\n",
    "\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the columns retrieved from the database are returned as all lowercase; however, when working with the raw data, these columns are capitalized. To maintain the fidelity of the original data, we will be using a function to change the imported column names back to their original formats using the function column_capitalizer from ColumnCapitals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = column_capitalizer(playlist, 'playlist')\n",
    "playlist.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PlayKey will be used as the Key to merge the datasets, so PlayerKey and GameID can be removed. While FieldType information is also in the surface column of the injuries table, we need to maintain it here, so we don't lose the data from the columns not containing injuries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.drop(columns=['PlayerKey', 'GameID'], inplace=True)\n",
    "playlist.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = playlist.dtypes[playlist.dtypes == 'object'].index.tolist()\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PlayKeys represent all plays, not only those where injuries occurred - these will function to merge the tables\n",
    "- FieldType only has 2 values, Natural or Synthetic and can be easily changed to binary values \n",
    "- Stadium Type is also strange with 29 unique types of stadiums. These can likely be grouped in smaller categories.\n",
    "- Weather - there are 63 unique types of weather.... this is odd. \n",
    "- RosterPosition, Position, and Position Group are all similar and need to be investigated\n",
    "- PlayTypes should be encoded, as they are categorical such as pass, rush, kick, ... \n",
    "\n",
    "---\n",
    "### Change the Field Types to Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function to change the surface values\n",
    "def surface_code(row):\n",
    "    surface = row['FieldType']\n",
    "    coded_surface = 0\n",
    "    if surface == 'Natural':\n",
    "        coded_surface = 0\n",
    "    elif surface == 'Synthetic':\n",
    "        coded_surface = 1\n",
    "\n",
    "    return coded_surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is later grouped into the InjuryCleaning.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist['CodedSurface'] = playlist.apply(surface_code, axis=1)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist# The code above worked, now change the FieldType to the coded and remove the redundant column\n",
    "playlist['FieldType'] = playlist['CodedSurface']\n",
    "playlist.drop(columns='CodedSurface', inplace=True)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Reduce the Number of Stadium Types to Something Meaningful\n",
    "\n",
    "It turns out that there are a lot of misspelled stadium types. There are 7 unique spellings of the word 'Outdoor' alone. Also, the people of Pittsburgh seemed pretty confused as to the meaning of Stadium Type, as there are MANY entries listing the stadium type as Heinz Field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadiums = playlist.StadiumType.unique().tolist()\n",
    "stadiums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Stadium Types are missing? \n",
    "playlist.StadiumType.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since most stadiums are outdoor stadiums, for now, just going to change any NaN stadiums to outdoor for now\n",
    "playlist.StadiumType.fillna('Outdoor', inplace=True)\n",
    "playlist.StadiumType.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping all stadiums into Outdoor, Indoor, Open Dome, or Closed Dome using a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'Outdoor': 'Outdoor',\n",
    "        'Indoors': 'Indoor',\n",
    "        'Oudoor': 'Outdoor',\n",
    "        'Outdoors': 'Outdoor',\n",
    "        'Open': 'Outdoor',\n",
    "        'Closed Dome': 'Indoor',\n",
    "        'Domed, closed': 'Indoor',\n",
    "        'Dome': 'Indoor',\n",
    "        'Indoor': 'Indoor',\n",
    "        'Domed': 'Indoor',\n",
    "        'Retr. Roof-Closed': 'Indoor',\n",
    "        'Outdoor Retr Roof-Open': 'Outdoor',\n",
    "        'Retractable Roof': 'Indoor',\n",
    "        'Ourdoor': 'Outdoor',\n",
    "        'Indoor, Roof Closed': 'Indoor',\n",
    "        'Retr. Roof - Closed': 'Indoor',\n",
    "        'Bowl': 'Outdoor',\n",
    "        'Outddors': 'Outdoor',\n",
    "        'Retr. Roof-Open': 'Outdoor',\n",
    "        'Dome, closed': 'Indoor',\n",
    "        'Indoor, Open Roof': 'Outdoor',\n",
    "        'Domed, Open': 'Outdoor',\n",
    "        'Domed, open': 'Outdoor',\n",
    "        'Heinz Field': 'Outdoor',\n",
    "        'Cloudy': 'Outdoor',\n",
    "        'Retr. Roof - Open': 'Outdoor',\n",
    "        'Retr. Roof Closed': 'Indoor',\n",
    "        'Outdor': 'Outdoor',\n",
    "        'Outside': 'Outdoor'}\n",
    "\n",
    "\n",
    "playlist.StadiumType.replace(dict, inplace=True)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### For the Supervised Learning, going to initially group the Stadium Types as Outdoor, or Not Outdoor in a new column, OutdoorStadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the numpy where to classify anything that meets the True condition as 1, denoting Outdoor Stadium, and False becomes 0, for all other non-outdoor stadiums\n",
    "playlist['OutdoorStadium'] = np.where(playlist['StadiumType']=='Outdoor', 1, 0)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dealing with the Weather Situation\n",
    "\n",
    "There were a lot of different entries meaning the same thing; these were grouped in a dictionary the same way the stadiums were, and can be adjusted if necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {'Clear and warm': 'Clear',\n",
    "                'Mostly Cloudy': 'Cloudy',\n",
    "                'Sunny': 'Clear',\n",
    "                'Clear': 'Clear',\n",
    "                'Cloudy': 'Cloudy',\n",
    "                'Cloudy, fog started developing in 2nd quarter': 'Hazy/Fog',\n",
    "                'Rain': 'Rain',\n",
    "                'Partly Cloudy': 'Cloudy',\n",
    "                'Mostly cloudy': 'Cloudy',\n",
    "                'Cloudy and cold': 'Cloudy',\n",
    "                'Cloudy and Cool': 'Cloudy',\n",
    "                'Rain Chance 40%': 'Rain',\n",
    "                'Controlled Climate': 'Indoor',\n",
    "                'Sunny and warm': 'Clear',\n",
    "                'Partly cloudy': 'Cloudy',\n",
    "                'Clear and Cool': 'Cloudy',\n",
    "                'Clear and cold': 'Cloudy',\n",
    "                'Sunny and cold': 'Clear',\n",
    "                'Indoor': 'Indoor',\n",
    "                'Partly Sunny': 'Clear',\n",
    "                'N/A (Indoors)': 'Indoor',\n",
    "                'Mostly Sunny': 'Clear',\n",
    "                'Indoors': 'Indoor',\n",
    "                'Clear Skies': 'Clear',\n",
    "                'Partly sunny': 'Clear',\n",
    "                'Showers': 'Rain',\n",
    "                'N/A Indoor': 'Indoor',\n",
    "                'Sunny and clear': 'Clear',\n",
    "                'Snow': 'Snow',\n",
    "                'Scattered Showers': 'Rain',\n",
    "                'Party Cloudy': 'Cloudy',\n",
    "                'Clear skies': 'Clear',\n",
    "                'Rain likely, temps in low 40s.': 'Rain',\n",
    "                'Hazy': 'Hazy/Fog',\n",
    "                'Partly Clouidy': 'Cloudy',\n",
    "                'Sunny Skies': 'Clear',\n",
    "                'Overcast': 'Cloudy',\n",
    "                'Cloudy, 50% change of rain': 'Cloudy',\n",
    "                'Fair': 'Clear',\n",
    "                'Light Rain': 'Rain',\n",
    "                'Partly clear': 'Clear',\n",
    "                'Mostly Coudy': 'Cloudy',\n",
    "                '10% Chance of Rain': 'Cloudy',\n",
    "                'Cloudy, chance of rain': 'Cloudy',\n",
    "                'Heat Index 95': 'Clear',\n",
    "                'Sunny, highs to upper 80s': 'Clear',\n",
    "                'Sun & clouds': 'Cloudy',\n",
    "                'Heavy lake effect snow': 'Snow',\n",
    "                'Mostly sunny': 'Clear',\n",
    "                'Cloudy, Rain': 'Rain',\n",
    "                'Sunny, Windy': 'Windy',\n",
    "                'Mostly Sunny Skies': 'Clear',\n",
    "                'Rainy': 'Rain',\n",
    "                '30% Chance of Rain': 'Rain',\n",
    "                'Cloudy, light snow accumulating 1-3': 'Snow',\n",
    "                'cloudy': 'Cloudy',\n",
    "                'Clear and Sunny': 'Clear',\n",
    "                'Coudy': 'Cloudy',\n",
    "                'Clear and sunny': 'Clear',\n",
    "                'Clear to Partly Cloudy': 'Clear',\n",
    "                'Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.': 'Windy',\n",
    "                'Rain shower': 'Rain',\n",
    "                'Cold': 'Clear'}\n",
    "\n",
    "playlist.Weather.replace(weather_dict, inplace=True)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess whether the nan rows are indoor statiums, in which case, change to Indoor, otherwise remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Weather.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is impossible to predict the outdoor weather, we will have to drop the NaN values associated with outdoor, but the indoor NaN values can be filled with Indoor weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code identifies from the plays table, where the stadium type is 'Indoor' and then fills NaN values in the 'Weather' column with 'Indoor'.\n",
    "playlist.loc[playlist.StadiumType == 'Indoor',\n",
    "             'Weather'] = playlist.loc[playlist.StadiumType == 'Indoor', 'Weather'].fillna('Indoor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This addeda bout 7000 values to the Indoor values\n",
    "playlist.Weather.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining ~ 5,000 were outdoor with no weather - going to remove these since it's impossible to predict the weather conditions\n",
    "playlist.Weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of dropping over 18000 values, we only have to drop 5000 values due to NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible to determine the weather on those days if absolutely necessary, this looks like about 1.9% of the data...\n",
    "playlist = playlist.loc[playlist.Weather.isna() == False]\n",
    "playlist.Weather.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather has been reduced from 63 different values to 7\n",
    "playlist.Weather.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Weather has been reduced to fewer than 10, it is ready to be encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Encoding the Weather in a new column called Precipitation\n",
    "Most of the documented material from online sources suggest that the only weather that really has a big effect on play is the presence of precipitation in the form of rain or snow. \n",
    "\n",
    "Weather can be ranked in order of impact:  Clear and Indoor= 0, Cloudy = 0,  Windy = 0, Hazy/Fog = 0, Rain = 1, Snow = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation = {\n",
    "    'Indoor': 0, \n",
    "    'Clear': 0, \n",
    "    'Cloudy': 0,\n",
    "    'Windy': 0,\n",
    "    'Hazy/Fog': 0, \n",
    "    'Rain': 1, \n",
    "    'Snow': 1 \n",
    "}\n",
    "\n",
    "playlist['Precipitation'] = playlist.Weather.map(precipitation)\n",
    "playlist.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Looking at the Temperature Values - was determined in the PCA that some temperatures were... aberrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Temperature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist['Temperature'] = np.where(\n",
    "    (playlist['Temperature'] == -999) & (playlist['StadiumType'] == 'Indoor'), 70, playlist.Temperature)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Temperature.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 18000 temperatures were included as -999 degrees. This did impact the analysis, and for the time being, these will all be dropped for initial analysis.\n",
    "Almost all of the -999 degree measurements were from indoor stadiums, most of which have a set temperature to 70 degrees, so -999 was set to 70 for all indoor stadiums. \n",
    "\n",
    "There were only 807 rows of the 18000 that remain from outdoor stadiums that will be removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = playlist[playlist['Temperature'] != -999]\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Addressing the Positions Issue\n",
    "\n",
    "RosterPositions are similar to the PositionGroups, only not put in abbreviations. Will need to change the Roster Positions into abbreviations first. PositionGroups can be dropped, since they are nearly identical to the Roster and actual positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.RosterPosition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Position.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to change the the positions to numerical dummy values for the machine learning analysis instead of using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_dict = {\n",
    "    'Quarterback': 0,\n",
    "    'Wide Receiver': 1,\n",
    "    'Linebacker': 2,\n",
    "    'Running Back': 3,\n",
    "    'Defensive Lineman': 4,\n",
    "    'Tight End': 5,\n",
    "    'Safety': 6,\n",
    "    'Cornerback': 7,\n",
    "    'Offensive Lineman': 8,\n",
    "    'Kicker': 9\n",
    "}\n",
    "\n",
    "playlist.RosterPosition.replace(position_dict, inplace=True)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Position.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Position Group column\n",
    "playlist = playlist.drop(columns='PositionGroup')\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Position[playlist.Position == \"Missing Data\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code identifies \"Missing Data\" from the Position and replaces the missing value with the RosterPosition\n",
    "playlist['Position'] = np.where(playlist['Position'] == 'Missing Data', playlist['RosterPosition'], playlist['Position'])\n",
    "\n",
    "# Verify that the missing Data values have been replaced\n",
    "playlist.Position[playlist.Position == \"Missing Data\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.Position.value_counts()\n",
    "# This is binned into more than 10 groups and may not produce reliable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.RosterPosition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above values show how many recorded plays each player type was logged in as for all data. The positions are categorical and will be encoded using OneHotEncoder, changing them to binary columns. The Roster Position is the general class, and is redudant if we keep both position and Roster Position.\n",
    "\n",
    "Position was initially tested, and only the WR and OLB had a high impact and were related to the frequency of the positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something weird happened when trying to do a Naive Bayes... it found negative values... \n",
    "min(playlist.PlayerDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = playlist.assign(DaysPlayed = lambda x: x['PlayerDay'] + 63)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(playlist.DaysPlayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.drop(columns='PlayerDay', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleaning The Injuries Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the specific table - this can be done on the same connection:\n",
    "injuries_sql = db.Table('injuries', metadata,\n",
    "                        autoload=True, autoload_with=engine)\n",
    "query = db.select(injuries_sql)\n",
    "Results = conn.execute(query).fetchall()\n",
    "\n",
    "# Create the new dataframe and set the keys\n",
    "injuries = pd.DataFrame(Results)\n",
    "injuries.columns = Results[0].keys()\n",
    "conn.close()\n",
    "del Results, metadata, conn, engine, query, table, db_string\n",
    "injuries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injuries = column_capitalizer(injuries, 'injuries')\n",
    "injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all columns for na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PlayKey column is the only one that has NaN values\n",
    "injuries['PlayKey'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the NaN values, since we won't be able to correlate these with the other tables\n",
    "injuries = injuries.dropna(subset = ['PlayKey'])\n",
    "injuries.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is only 1 unique value for DM_M1 - which means that every player on this list was injured for at least 1 day\n",
    "\n",
    "The Surface is the same as the Field Type from the other table, so this can be dropped. \n",
    "Note: Anyone whose injury is in the DM_M42 list is also in all of the prior lists, so there will be more of the lower values due the the encoding. Going to change this to a single column with values of 1, 7, 28, and 42\n",
    "\n",
    "---\n",
    "### Group the DM columns into a single Injury Duration column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def injury_duration(row):\n",
    "    injury_duration = 0\n",
    "    if row[\"DM_M42\"] == 1:\n",
    "        injury_duration = 42\n",
    "    else:\n",
    "        if row[\"DM_M28\"] == 1:\n",
    "            injury_duration = 28\n",
    "        else:\n",
    "            if row[\"DM_M7\"] == 1:\n",
    "                injury_duration = 7\n",
    "            else: \n",
    "                injury_duration = 1\n",
    "    \n",
    "    return injury_duration\n",
    "\n",
    "# Apply the function to all rows\n",
    "injuries['InjuryDuration'] = injuries.apply(injury_duration, axis=1)\n",
    "injuries.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows for DMs\n",
    "injuries.drop(columns=['DM_M1', 'DM_M7', 'DM_M28', 'DM_M42', 'Surface'], inplace=True)\n",
    "injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the BodyPart of injury to verify it's ready for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The body parts are categorical and will be, but since each injury was logged as unique, going to use the occurrence frequency as the numerical coding instead of arbitrary numbers\n",
    "knee_freq = injuries.BodyPart.value_counts()['Knee']\n",
    "ankle_freq = injuries.BodyPart.value_counts()['Ankle']\n",
    "foot_freq = injuries.BodyPart.value_counts()['Foot']\n",
    "injuries.BodyPart.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 74 known individual players that have been injured for at least 1 day \n",
    "injuries.PlayerKey.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This output only 76 unique plays with only 74 players, so only 2 players were reinjured at different times of the season\n",
    "injuries.PlayKey.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every GameID and PlayID are unique, meaning that once that\n",
    "particular player was injured during a specific game at a specific play,\n",
    "they didn't return to the field. Since the GameID numbers are not in any \n",
    "chronological order and offer no information other than the PlayKey can, this column can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injuries.GameID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the PlayerID, GameID, and PlayKey number are all contained within the PlayKey, the GameID and PlayerID can be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injuries.drop(columns=['GameID', 'PlayerKey'], inplace=True)\n",
    "injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the supervised analysis, the injuries will need to be recorded as numerical values. We will create 2 columns:\n",
    "- 'IsInjured' where 0 is not injured and 1 is injured\n",
    "- 'InjuryType' where the Injury Type will be encoded by the frequency of occurrence, Knee = 36, Ankle = 35, and Foot = 6\n",
    "\n",
    "Depeding on the type of analysis, if we're trying to predict with a binary outcome, whether or not there will be an injury, we will use 'IsInjured'. If we're trying to predict which types of injury, we'd need the numerical factors for each type of injury. \n",
    "\n",
    "These changes cannot be made until this table is merged with the other table, containing the non-injured player plays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Merge the 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop the categorical columns that have been encoded for the supervised analysis\n",
    "- Play Type and RosterPosition will be encoded with OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.drop(columns=['StadiumType', 'Weather', 'Position'], inplace=True)\n",
    "playlist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can switch out the RosterPosition for the played position to see if there was a difference, but the actual position is more specific to the play, which may be a better indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injuries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries = pd.merge(playlist, injuries, on='PlayKey', how='outer')\n",
    "play_injuries_inner = pd.merge(playlist, injuries, on='PlayKey', how='inner')\n",
    "play_injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Add values for duration and Body Part. Change NaN to NoInjury for body part. Change Injury_Duration to 0 for all NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries.BodyPart.fillna('NoInjury', inplace=True)\n",
    "play_injuries.InjuryDuration.fillna(0, inplace=True)\n",
    "play_injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a binary column for injury/no_injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries['IsInjured'] = play_injuries['BodyPart'].apply(lambda x: 0 if x == 'NoInjury'  else 1)\n",
    "play_injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the numerical frequency-based column for the InjuryTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the columns \n",
    "injury_type = {\n",
    "    'Knee': knee_freq, \n",
    "    'Ankle': ankle_freq,\n",
    "    'Foot': foot_freq, \n",
    "    'NoInjury': 0\n",
    "}\n",
    "\n",
    "play_injuries['InjuryType'] = play_injuries.BodyPart.map(injury_type)\n",
    "play_injuries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries.InjuryType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the BodyPart column and PlayKey\n",
    "play_injuries.drop(columns=['PlayKey','BodyPart'], inplace=True)\n",
    "play_injuries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be no na values in most of the columns, except the play type - where there are 336. This won't remove a lot of the data, so going to drop the remaining nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries = play_injuries.dropna()\n",
    "play_injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first run, the Pass and Rush plays had a large impact, but each of the other plays were minimal, and all of the other plays are effectively kicking plays. Going to group the other plays to reduce features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries.PlayType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_type = {\n",
    "    'Pass': 'Pass',\n",
    "    'Rush': 'Rush',\n",
    "    'Extra Point': 'Kick',\n",
    "    'Kickoff': 'Kick',\n",
    "    'Punt': 'Kick',\n",
    "    'Field Goal': 'Kick',\n",
    "    'Kickoff Not Returned': 'Kick',\n",
    "    'Punt Not Returned': 'Kick',\n",
    "    'Kickoff Returned': 'Kick',\n",
    "    'Punt Returned': 'Kick',\n",
    "    '0': 'Kick'\n",
    "}\n",
    "\n",
    "play_injuries['PlayType'] = play_injuries.PlayType.map(play_type)\n",
    "play_injuries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries.PlayType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_injuries.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Encode the Position and Play type using OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to encode all of the categorical data to be able to do a machine learning analysis. We opted to use OneHotEncoder in this case because of the possibility of non-existent categories in the testing, that may create a dimensional mismatch that OneHotEncoder can handle, but get_dummies cannot. Either way, the results end up being the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the categorical variables\n",
    "play_cat = play_injuries.dtypes[play_injuries.dtypes == 'object'].index.tolist()\n",
    "\n",
    "# Create the Encoder Instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the categorical data\n",
    "encode_df = pd.DataFrame(enc.fit_transform(play_injuries[play_cat]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names_out(play_cat)\n",
    "encode_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge encoded features and drop the original columns\n",
    "clean_play_injuries = play_injuries.merge(encode_df, left_index=True, right_index=True)\n",
    "clean_play_injuries.drop(columns=play_cat, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_play_injuries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Data for Analysis\n",
    "\n",
    "This will export the cleaned data to a new database called NFL_Injuries, which will be used for all of the processed data, keeping it separated from the original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make connection to the database\n",
    "from config import db_password\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5433/NFL_Injuries\"\n",
    "engine = db.create_engine(db_string)\n",
    "\n",
    "del db_string, db_password\n",
    "# Write table to database\n",
    "# clean_play_injuries.to_sql(name='clean_play_injuries', con=engine, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55a128abeef29120eb7bd22a6a2e883184ae9f9178564f0df9c7c7d3d676105f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
